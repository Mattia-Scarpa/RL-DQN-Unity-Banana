{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Environment and Agent\n",
    "\n",
    "The first step will be to load the environment built on unity, The we exctract \"_brain_\" information, which is the responsible of taking action in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Banana_Linux/Banana.x86_64\")\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the information of the environment we get that the agent has at its disposal 4 action given by:\n",
    "\n",
    "- `0` - walk forward;\n",
    "- `1` - walk backward;\n",
    "- `2` - turn left;\n",
    "- `3` - turn right.\n",
    "\n",
    "On the other hand the state is made of a 37-dimensional space containing the agent velocity of a ray-based perception for all the objects in the environment around the agent's forward direction.\n",
    "\n",
    "The first step brefore to train our agent is to reset the environment, namely placing it in a random point in its available square space. The final goal for the agent will be to collect as much yellow banana as possibile (which will provide a reward of +1), while avoiding the blue banana (returning a penalty of -1).\n",
    "\n",
    "The environment will be considered solved if the agent will collect at least an average overall reaward &ge;13 along 10 episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time to import the agent responsible to learn the task just described. \n",
    "In order to solve the task the idea is to implement a Deep Q-Network (DQN) that, at each time step or at every pre defined step interval, a neuarl networks (NN) aims to learn the best action, or more formally aims to approximate the state action-value providing a tool to find the more valuable action, to apply at that time istant provided the 37-dimensional state input.\n",
    "\n",
    "At each optimization step the agent will take the action according to the model prediction and will exploit the obtained reward to optimize the NN weigths exploiting the gradiend descent technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqn_agent import Agent\n",
    "\n",
    "\n",
    "action_size = brain.vector_action_space_size\n",
    "state_size = len(env_info.vector_observations[0])\n",
    "\n",
    "agent = Agent(state_size=state_size, action_size=action_size, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Training\n",
    "\n",
    "We must be aware that DQN tends to overestimate action values. To avoid this issues, and to improve the learning phase, we apply some techniques that has been shown to be the more prominent ones.\n",
    "Specifically we will resot to **_Double DQN_** (DDQN), **_Prioritized Experience Replay_** (PER) and, last but not least, **_Dueling DQN_** which combined with DDQN becomes **_Dueling DDQN_**.\n",
    "\n",
    "1. ##### Double DQN\n",
    "Double Q-Learning has shown great performances to avoid the issues of action values overestimation. To do so we start by analyzing the update rule for Q-Learning with NN function approximation, that is:\n",
    "$$\\omega_{i+1} = \\omega_i + \\alpha \\left( R + \\gamma \\max_a Q(s', a'; \\omega_i) - Q(s, a; \\omega_i) \\right) \\nabla_{\\omega_i} Q(s, a; \\omega_i)$$\n",
    "with TD target given by $R + \\gamma \\max_a Q(s', a'; \\omega_i)$ or $R + \\gamma Q(s', argmax_aQ(s', a, \\omega), \\omega)$. Here we can see that if $\\omega$ are not well tuned, for instance at early stage, it might be easy to make mistakes.\n",
    "To overcome this issues in the evaluation of the TD target we will resort to a different set of weights $\\omega^-_i$ that will be updated parallely to the main weights according to the rule given by:\n",
    "$$\\omega^-_i = \\tau * \\omega_i + (1 - \\tau) * \\omega^-_i$$\n",
    "where $\\tau$ is a coefficent that aim to regularize the target update according to the local NN. Namely, if $\\tau = 1$ means that the two networks coincide, and thus the Double DQN becomes a simple DQN, while if $\\tau = 0$ the target NN is not getting updated. This parameter might be an hyperparameters interesting to tune or even changing along the training. However in this project it has been chosen fixed.\n",
    "\n",
    "2. ##### Prioritized Experience Replay\n",
    "In traditional deep reinforcement learning, Experience Replay enhances training by storing and randomly sampling past experiences. However, treating all experiences as equal can be inefficient. For this reason we will exploit Prioritized Experience Replay (PER).\n",
    "PER revolutionizes the sampling process by assigning priorities to experiences based on their temporal difference (TD) errors. Larger TD errors signal unexpected or significant experiences, making them prime candidates for relearning. Instead of sampling uniformly, PER adjusts the probability of selecting a particular experience based on its TD error. Thus, experiences that offer more learning potential are revisited more often.\n",
    "This non-uniform sampling, while efficient, introduces bias. To counteract this, importance-sampling weights are utilized, ensuring unbiased learning updates. In essence, PER combines the best of both worlds: the efficiency of prioritizing valuable experiences and the rigor of unbiased learning.\n",
    "\n",
    "3. ##### Dueling DDQN\n",
    "Dueling DQN introduces an architectural nuance to the DQN's neural network. Rather than directly estimating Q-values, it decouples the state values and the advantages of each action.\n",
    "In a traditional setup, the neural network provides Q-values for actions given a state. With Dueling DQN, the architecture divides the final layer into two pathways: one for the state value function $ V(s; \\omega) $ and another for the advantage function $ A(s, a; \\omega) $ of each action. These are combined to produce Q-values:\n",
    "$$Q(s, a; \\omega) = V(s; \\omega) + A(s, a; \\omega)$$\n",
    "By separating state value and action advantages, Dueling DQN allows for a more nuanced learning process, leading to potentially faster and more stable policies.\n",
    "\n",
    "#### Other Hyperparameters\n",
    "\n",
    "1. **DDQN Coefficent**: As previously state $\\tau$ is the responsible on tuning the DDQN approach. For this experiment we set the values fixed to $\\tau=5 \\times 10^{-3}$.\n",
    "2. **Learning Rate**: In the DQN's neural network, the learning rate (LR) determines the step size taken towards minimizing the loss during each update. Set initially to $1 \\times 10^{-3}$, it dictates how aggressively our model adapts to new information. Furthermore, the learning rate decays at a rate of 0.9999 after each update, gradually reducing the step size and ensuring more refined learning as training progresses.\n",
    "3. **Discount Factor**: The discount factor $ \\gamma $, set at 0.95, influences how the DQN values future rewards in comparison to immediate ones. A value closer to 1 means the agent gives substantial importance to long-term rewards, ensuring that it's motivated to make decisions beneficial in the long run. This fit our case since the majority of the action are not supposed to return a positive reward or just only a reward different from 0.\n",
    "\n",
    "4. **Exploration vs Exploitation**: In DQL, or in general for all RL framework, exploration vs exploitation is a critical balance to maintain. The parameter $\\varepsilon$ dictates this balance. At the beginning of training, our agent knows little about its environment. Thus, it's beneficial to explore as much as possible, and this is facilitated by setting a high initial $\\varepsilon$ value, $\\varepsilon_{start} = 1.0$. This means the agent will predominantly take random actions, exploring the vastness of its state-action space.\n",
    "However, as the agent interacts more with its environment and starts to learn, it's crucial to decrease the randomness and lean more towards exploiting what it has already learned. The $\\varepsilon_{decay}$ factor, set at $0.999$, ensures a gradual decrease in the value of $\\varepsilon$ after each episode, thus slowly transitioning the agent from an exploration-centric mode to an exploitation one.\n",
    "Despite the importance of exploitation in the later stages of training, it's vital to retain some level of exploration throughout. This ensures that the agent doesn't become overly myopic and miss out on potential better strategies. The $\\varepsilon_{end}$ parameter, set to $0.01$, is the floor for $\\varepsilon$. It guarantees that the agent will always maintain a $1\\%$ chance of taking a random action, preserving some exploratory behavior throughout its lifetime.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(n_episode = 1800, max_t= 1000, eps_start=1., eps_end=.01, eps_decay=.995):\n",
    "\n",
    "    \"\"\"Deep Q-Learning.\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    scores = []                         # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)    # last 10 scores\n",
    "    eps = eps_start                     # initialize epsilon\n",
    "    best_scores = 11.0\n",
    "\n",
    "    for i_episode in range(1, n_episode+1):\n",
    "        state = env.reset(train_mode=True)[brain_name].vector_observations[0] # reset the environment\n",
    "        #print(state)\n",
    "        score = 0\n",
    "\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps) # select an action\n",
    "            \n",
    "            env_info = env.step(action)[brain_name] # send the action to the environment\n",
    "\n",
    "            next_state = env_info.vector_observations[0] # get the next state\n",
    "            reward = env_info.rewards[0]\n",
    "            # change the reward to -0.01 if the agent doesn't get the banana\n",
    "            if reward == 0:\n",
    "                reward = -0.01\n",
    "            done = env_info.local_done[0]\n",
    "\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "\n",
    "            score += reward # update the score\n",
    "            state = next_state # roll over the state to next time step\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        scores_window.append(score) # save most recent score\n",
    "        scores.append(score)\n",
    "        eps = max(eps_end, eps_decay*eps)\n",
    "\n",
    "        if i_episode % 10 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}, with epsilon:{:.5f}'.format(i_episode, np.mean(scores_window), eps))\n",
    "        if np.mean(scores_window)>=best_scores:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'best_model.pth')\n",
    "            best_scores = np.mean(scores_window)\n",
    "\n",
    "            \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning algorithm itself run the agent iteratively through repeated episode. Each episode is give by a total of 300 steps allowed. At each steps:\n",
    "\n",
    "1. The agent take the action according to the currently loaded model and a $\\epsilon$-greedy policy. \n",
    "2. The action selected by the agest is sent to the environment and the information are stored for next evaluation. More specifically we are interested in the **_next_state_** reached, the **_reward_** returned, and the **_done_** flag that indicate if the episode is terminated or not.\n",
    "3. The next step is a slight modification of the \"reward framework\", since we have modified the reward function to return a value of $-0.01$ rather than $0$ to incentivate the agent to loking for collecting banana. This has been done after some observation hoping to prevent the agent to get stucked sometimes in weird loop without taking any meaningful action.\n",
    "4. Subsequently the agent will take its step by adding the current environment experience in its PER Buffer and once reached a minimun quantity of episode saved in its memory starting the learning phase. This procedure, and specifically the model update, is done every $4$ steps, according to the hyperparameter, and basically it consist in sampling a **_BATCH_SIZE_** number of episode randomly according to the probability defined by the TD error for each episode. Then using that sample set to update the NN model and \"soft-updateing\" the target model.\n",
    "5. Finally store the score obtained combining the reward of the episode, and as last update the new $\\epsilon$ reducing the randomness for action decision process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tAverage Score: -2.89, with epsilon:0.95111\n",
      "Episode 20\tAverage Score: -3.13, with epsilon:0.90461\n",
      "Episode 30\tAverage Score: -3.29, with epsilon:0.86038\n",
      "Episode 40\tAverage Score: -3.36, with epsilon:0.81832\n",
      "Episode 50\tAverage Score: -3.16, with epsilon:0.77831\n",
      "Episode 60\tAverage Score: -2.93, with epsilon:0.74026\n",
      "Episode 70\tAverage Score: -2.81, with epsilon:0.70407\n",
      "Episode 80\tAverage Score: -2.46, with epsilon:0.66965\n",
      "Episode 90\tAverage Score: -2.13, with epsilon:0.63691\n",
      "Episode 100\tAverage Score: -1.85, with epsilon:0.60577\n",
      "Episode 110\tAverage Score: -1.55, with epsilon:0.57615\n",
      "Episode 120\tAverage Score: -1.08, with epsilon:0.54799\n",
      "Episode 130\tAverage Score: -0.50, with epsilon:0.52120\n",
      "Episode 140\tAverage Score: -0.03, with epsilon:0.49571\n",
      "Episode 150\tAverage Score: 0.42, with epsilon:0.47148\n",
      "Episode 160\tAverage Score: 1.03, with epsilon:0.44843\n",
      "Episode 170\tAverage Score: 1.55, with epsilon:0.42650\n",
      "Episode 180\tAverage Score: 1.75, with epsilon:0.40565\n",
      "Episode 190\tAverage Score: 2.14, with epsilon:0.38582\n",
      "Episode 200\tAverage Score: 2.41, with epsilon:0.36696\n",
      "Episode 210\tAverage Score: 2.82, with epsilon:0.34902\n",
      "Episode 220\tAverage Score: 3.23, with epsilon:0.33195\n",
      "Episode 230\tAverage Score: 3.42, with epsilon:0.31572\n",
      "Episode 240\tAverage Score: 3.72, with epsilon:0.30029\n",
      "Episode 250\tAverage Score: 4.18, with epsilon:0.28561\n",
      "Episode 260\tAverage Score: 4.16, with epsilon:0.27164\n",
      "Episode 270\tAverage Score: 4.40, with epsilon:0.25836\n",
      "Episode 280\tAverage Score: 4.90, with epsilon:0.24573\n",
      "Episode 290\tAverage Score: 5.13, with epsilon:0.23372\n",
      "Episode 300\tAverage Score: 5.37, with epsilon:0.22229\n",
      "Episode 310\tAverage Score: 5.66, with epsilon:0.21142\n",
      "Episode 320\tAverage Score: 5.83, with epsilon:0.20109\n",
      "Episode 330\tAverage Score: 6.09, with epsilon:0.19126\n",
      "Episode 340\tAverage Score: 6.57, with epsilon:0.18191\n",
      "Episode 350\tAverage Score: 6.87, with epsilon:0.17301\n",
      "Episode 360\tAverage Score: 7.36, with epsilon:0.16455\n",
      "Episode 370\tAverage Score: 7.42, with epsilon:0.15651\n",
      "Episode 380\tAverage Score: 7.55, with epsilon:0.14886\n",
      "Episode 390\tAverage Score: 7.59, with epsilon:0.14158\n",
      "Episode 400\tAverage Score: 7.98, with epsilon:0.13466\n",
      "Episode 410\tAverage Score: 8.23, with epsilon:0.12807\n",
      "Episode 420\tAverage Score: 8.43, with epsilon:0.12181\n",
      "Episode 430\tAverage Score: 8.66, with epsilon:0.11586\n",
      "Episode 440\tAverage Score: 8.87, with epsilon:0.11019\n",
      "Episode 450\tAverage Score: 8.86, with epsilon:0.10481\n",
      "Episode 460\tAverage Score: 9.01, with epsilon:0.09968\n",
      "Episode 470\tAverage Score: 9.38, with epsilon:0.09481\n",
      "Episode 480\tAverage Score: 9.48, with epsilon:0.09017\n",
      "Episode 490\tAverage Score: 9.73, with epsilon:0.08576\n",
      "Episode 500\tAverage Score: 9.89, with epsilon:0.08157\n",
      "Episode 510\tAverage Score: 10.05, with epsilon:0.07758\n",
      "Episode 520\tAverage Score: 10.49, with epsilon:0.07379\n",
      "Episode 530\tAverage Score: 10.64, with epsilon:0.07018\n",
      "Episode 540\tAverage Score: 10.61, with epsilon:0.06675\n",
      "Episode 550\tAverage Score: 10.79, with epsilon:0.06349\n",
      "Episode 560\tAverage Score: 11.01, with epsilon:0.06038\n",
      "\n",
      "Environment solved in 560 episodes!\tAverage Score: 11.01\n",
      "\n",
      "Environment solved in 561 episodes!\tAverage Score: 11.08\n",
      "\n",
      "Environment solved in 564 episodes!\tAverage Score: 11.12\n",
      "\n",
      "Environment solved in 565 episodes!\tAverage Score: 11.19\n",
      "\n",
      "Environment solved in 567 episodes!\tAverage Score: 11.22\n",
      "\n",
      "Environment solved in 568 episodes!\tAverage Score: 11.26\n",
      "Episode 570\tAverage Score: 11.26, with epsilon:0.05743\n",
      "\n",
      "Environment solved in 570 episodes!\tAverage Score: 11.26\n",
      "\n",
      "Environment solved in 572 episodes!\tAverage Score: 11.27\n",
      "\n",
      "Environment solved in 573 episodes!\tAverage Score: 11.34\n",
      "\n",
      "Environment solved in 575 episodes!\tAverage Score: 11.38\n",
      "\n",
      "Environment solved in 576 episodes!\tAverage Score: 11.41\n",
      "\n",
      "Environment solved in 577 episodes!\tAverage Score: 11.44\n",
      "\n",
      "Environment solved in 578 episodes!\tAverage Score: 11.48\n",
      "\n",
      "Environment solved in 579 episodes!\tAverage Score: 11.53\n",
      "Episode 580\tAverage Score: 11.54, with epsilon:0.05462\n",
      "\n",
      "Environment solved in 580 episodes!\tAverage Score: 11.54\n",
      "\n",
      "Environment solved in 581 episodes!\tAverage Score: 11.61\n",
      "\n",
      "Environment solved in 584 episodes!\tAverage Score: 11.63\n",
      "\n",
      "Environment solved in 585 episodes!\tAverage Score: 11.65\n",
      "\n",
      "Environment solved in 586 episodes!\tAverage Score: 11.67\n",
      "\n",
      "Environment solved in 588 episodes!\tAverage Score: 11.69\n",
      "\n",
      "Environment solved in 589 episodes!\tAverage Score: 11.72\n",
      "Episode 590\tAverage Score: 11.80, with epsilon:0.05195\n",
      "\n",
      "Environment solved in 590 episodes!\tAverage Score: 11.80\n",
      "\n",
      "Environment solved in 591 episodes!\tAverage Score: 11.83\n",
      "Episode 600\tAverage Score: 11.65, with epsilon:0.04941\n",
      "\n",
      "Environment solved in 607 episodes!\tAverage Score: 11.84\n",
      "\n",
      "Environment solved in 608 episodes!\tAverage Score: 11.95\n",
      "\n",
      "Environment solved in 609 episodes!\tAverage Score: 12.12\n",
      "Episode 610\tAverage Score: 12.05, with epsilon:0.04700\n",
      "Episode 620\tAverage Score: 11.84, with epsilon:0.04470\n",
      "Episode 630\tAverage Score: 11.98, with epsilon:0.04251\n",
      "Episode 640\tAverage Score: 12.05, with epsilon:0.04044\n",
      "Episode 650\tAverage Score: 11.90, with epsilon:0.03846\n",
      "Episode 660\tAverage Score: 11.91, with epsilon:0.03658\n",
      "Episode 670\tAverage Score: 11.69, with epsilon:0.03479\n",
      "Episode 680\tAverage Score: 11.67, with epsilon:0.03309\n",
      "Episode 690\tAverage Score: 11.76, with epsilon:0.03147\n",
      "Episode 700\tAverage Score: 11.79, with epsilon:0.02993\n",
      "Episode 710\tAverage Score: 11.48, with epsilon:0.02847\n",
      "Episode 720\tAverage Score: 11.72, with epsilon:0.02708\n",
      "Episode 730\tAverage Score: 11.65, with epsilon:0.02575\n",
      "Episode 740\tAverage Score: 11.85, with epsilon:0.02450\n",
      "\n",
      "Environment solved in 748 episodes!\tAverage Score: 12.12\n",
      "Episode 750\tAverage Score: 12.11, with epsilon:0.02330\n",
      "Episode 760\tAverage Score: 11.91, with epsilon:0.02216\n",
      "\n",
      "Environment solved in 766 episodes!\tAverage Score: 12.14\n",
      "\n",
      "Environment solved in 767 episodes!\tAverage Score: 12.16\n",
      "\n",
      "Environment solved in 768 episodes!\tAverage Score: 12.21\n",
      "\n",
      "Environment solved in 769 episodes!\tAverage Score: 12.30\n",
      "Episode 770\tAverage Score: 12.30, with epsilon:0.02108\n",
      "\n",
      "Environment solved in 775 episodes!\tAverage Score: 12.31\n",
      "\n",
      "Environment solved in 776 episodes!\tAverage Score: 12.32\n",
      "\n",
      "Environment solved in 777 episodes!\tAverage Score: 12.33\n",
      "\n",
      "Environment solved in 778 episodes!\tAverage Score: 12.33\n",
      "\n",
      "Environment solved in 779 episodes!\tAverage Score: 12.41\n",
      "Episode 780\tAverage Score: 12.39, with epsilon:0.02004\n",
      "\n",
      "Environment solved in 781 episodes!\tAverage Score: 12.44\n",
      "\n",
      "Environment solved in 782 episodes!\tAverage Score: 12.45\n",
      "\n",
      "Environment solved in 784 episodes!\tAverage Score: 12.48\n",
      "Episode 790\tAverage Score: 12.30, with epsilon:0.01906\n",
      "\n",
      "Environment solved in 797 episodes!\tAverage Score: 12.50\n",
      "\n",
      "Environment solved in 798 episodes!\tAverage Score: 12.58\n",
      "\n",
      "Environment solved in 799 episodes!\tAverage Score: 12.62\n",
      "Episode 800\tAverage Score: 12.65, with epsilon:0.01813\n",
      "\n",
      "Environment solved in 800 episodes!\tAverage Score: 12.65\n",
      "\n",
      "Environment solved in 801 episodes!\tAverage Score: 12.79\n",
      "\n",
      "Environment solved in 803 episodes!\tAverage Score: 12.83\n",
      "Episode 810\tAverage Score: 12.77, with epsilon:0.01725\n",
      "Episode 820\tAverage Score: 12.42, with epsilon:0.01640\n",
      "Episode 830\tAverage Score: 12.65, with epsilon:0.01560\n",
      "Episode 840\tAverage Score: 12.55, with epsilon:0.01484\n",
      "Episode 850\tAverage Score: 12.54, with epsilon:0.01411\n",
      "Episode 860\tAverage Score: 12.59, with epsilon:0.01342\n",
      "Episode 870\tAverage Score: 12.55, with epsilon:0.01277\n",
      "Episode 880\tAverage Score: 12.46, with epsilon:0.01214\n",
      "Episode 890\tAverage Score: 12.65, with epsilon:0.01155\n",
      "Episode 900\tAverage Score: 12.55, with epsilon:0.01098\n",
      "Episode 910\tAverage Score: 12.64, with epsilon:0.01045\n",
      "\n",
      "Environment solved in 914 episodes!\tAverage Score: 12.85\n",
      "Episode 920\tAverage Score: 12.70, with epsilon:0.01000\n",
      "Episode 930\tAverage Score: 12.42, with epsilon:0.01000\n",
      "Episode 940\tAverage Score: 12.50, with epsilon:0.01000\n",
      "Episode 950\tAverage Score: 12.48, with epsilon:0.01000\n",
      "Episode 960\tAverage Score: 12.74, with epsilon:0.01000\n",
      "Episode 970\tAverage Score: 12.63, with epsilon:0.01000\n",
      "Episode 980\tAverage Score: 12.58, with epsilon:0.01000\n",
      "Episode 990\tAverage Score: 12.52, with epsilon:0.01000\n",
      "Episode 1000\tAverage Score: 12.65, with epsilon:0.01000\n",
      "Episode 1010\tAverage Score: 12.33, with epsilon:0.01000\n",
      "Episode 1020\tAverage Score: 12.36, with epsilon:0.01000\n",
      "Episode 1030\tAverage Score: 12.61, with epsilon:0.01000\n",
      "Episode 1040\tAverage Score: 12.56, with epsilon:0.01000\n",
      "Episode 1050\tAverage Score: 12.60, with epsilon:0.01000\n",
      "Episode 1060\tAverage Score: 12.29, with epsilon:0.01000\n",
      "Episode 1070\tAverage Score: 12.57, with epsilon:0.01000\n",
      "Episode 1080\tAverage Score: 12.77, with epsilon:0.01000\n",
      "Episode 1090\tAverage Score: 12.53, with epsilon:0.01000\n",
      "Episode 1100\tAverage Score: 12.48, with epsilon:0.01000\n",
      "Episode 1110\tAverage Score: 12.63, with epsilon:0.01000\n",
      "Episode 1120\tAverage Score: 12.76, with epsilon:0.01000\n",
      "\n",
      "Environment solved in 1121 episodes!\tAverage Score: 12.86\n",
      "\n",
      "Environment solved in 1122 episodes!\tAverage Score: 12.90\n",
      "\n",
      "Environment solved in 1123 episodes!\tAverage Score: 12.95\n",
      "Episode 1130\tAverage Score: 12.95, with epsilon:0.01000\n",
      "\n",
      "Environment solved in 1132 episodes!\tAverage Score: 12.98\n",
      "\n",
      "Environment solved in 1136 episodes!\tAverage Score: 13.10\n",
      "\n",
      "Environment solved in 1138 episodes!\tAverage Score: 13.11\n",
      "\n",
      "Environment solved in 1139 episodes!\tAverage Score: 13.19\n",
      "Episode 1140\tAverage Score: 13.14, with epsilon:0.01000\n",
      "\n",
      "Environment solved in 1143 episodes!\tAverage Score: 13.21\n",
      "\n",
      "Environment solved in 1145 episodes!\tAverage Score: 13.24\n",
      "Episode 1150\tAverage Score: 13.02, with epsilon:0.01000\n",
      "Episode 1160\tAverage Score: 13.25, with epsilon:0.01000\n",
      "\n",
      "Environment solved in 1160 episodes!\tAverage Score: 13.25\n",
      "\n",
      "Environment solved in 1161 episodes!\tAverage Score: 13.25\n",
      "Episode 1170\tAverage Score: 12.84, with epsilon:0.01000\n",
      "Episode 1180\tAverage Score: 12.82, with epsilon:0.01000\n",
      "Episode 1190\tAverage Score: 12.91, with epsilon:0.01000\n",
      "Episode 1200\tAverage Score: 12.74, with epsilon:0.01000\n",
      "Episode 1210\tAverage Score: 12.83, with epsilon:0.01000\n",
      "Episode 1220\tAverage Score: 12.98, with epsilon:0.01000\n",
      "Episode 1230\tAverage Score: 12.81, with epsilon:0.01000\n",
      "Episode 1240\tAverage Score: 12.61, with epsilon:0.01000\n",
      "Episode 1250\tAverage Score: 12.68, with epsilon:0.01000\n",
      "Episode 1260\tAverage Score: 12.92, with epsilon:0.01000\n",
      "Episode 1270\tAverage Score: 13.26, with epsilon:0.01000\n",
      "\n",
      "Environment solved in 1270 episodes!\tAverage Score: 13.26\n",
      "\n",
      "Environment solved in 1271 episodes!\tAverage Score: 13.29\n",
      "\n",
      "Environment solved in 1274 episodes!\tAverage Score: 13.41\n",
      "\n",
      "Environment solved in 1275 episodes!\tAverage Score: 13.57\n",
      "Episode 1280\tAverage Score: 13.21, with epsilon:0.01000\n",
      "Episode 1290\tAverage Score: 13.46, with epsilon:0.01000\n",
      "\n",
      "Environment solved in 1295 episodes!\tAverage Score: 13.58\n",
      "Episode 1300\tAverage Score: 13.52, with epsilon:0.01000\n",
      "\n",
      "Environment solved in 1307 episodes!\tAverage Score: 13.62\n",
      "\n",
      "Environment solved in 1309 episodes!\tAverage Score: 13.62\n",
      "Episode 1310\tAverage Score: 13.57, with epsilon:0.01000\n",
      "Episode 1320\tAverage Score: 13.54, with epsilon:0.01000\n",
      "\n",
      "Environment solved in 1324 episodes!\tAverage Score: 13.68\n",
      "Episode 1330\tAverage Score: 13.42, with epsilon:0.01000\n",
      "Episode 1340\tAverage Score: 13.28, with epsilon:0.01000\n",
      "Episode 1350\tAverage Score: 13.49, with epsilon:0.01000\n",
      "Episode 1360\tAverage Score: 13.30, with epsilon:0.01000\n",
      "Episode 1370\tAverage Score: 13.15, with epsilon:0.01000\n",
      "Episode 1380\tAverage Score: 12.93, with epsilon:0.01000\n",
      "Episode 1390\tAverage Score: 12.69, with epsilon:0.01000\n",
      "Episode 1400\tAverage Score: 12.77, with epsilon:0.01000\n",
      "Episode 1410\tAverage Score: 12.87, with epsilon:0.01000\n",
      "Episode 1420\tAverage Score: 12.57, with epsilon:0.01000\n",
      "Episode 1430\tAverage Score: 12.51, with epsilon:0.01000\n",
      "Episode 1440\tAverage Score: 12.78, with epsilon:0.01000\n",
      "Episode 1450\tAverage Score: 12.49, with epsilon:0.01000\n",
      "Episode 1460\tAverage Score: 12.33, with epsilon:0.01000\n",
      "Episode 1470\tAverage Score: 12.27, with epsilon:0.01000\n",
      "Episode 1480\tAverage Score: 12.75, with epsilon:0.01000\n",
      "Episode 1490\tAverage Score: 12.86, with epsilon:0.01000\n",
      "Episode 1500\tAverage Score: 12.79, with epsilon:0.01000\n",
      "Episode 1510\tAverage Score: 12.59, with epsilon:0.01000\n",
      "Episode 1520\tAverage Score: 12.73, with epsilon:0.01000\n",
      "Episode 1530\tAverage Score: 12.85, with epsilon:0.01000\n",
      "Episode 1540\tAverage Score: 12.98, with epsilon:0.01000\n",
      "Episode 1550\tAverage Score: 12.98, with epsilon:0.01000\n",
      "Episode 1560\tAverage Score: 13.20, with epsilon:0.01000\n",
      "Episode 1570\tAverage Score: 13.11, with epsilon:0.01000\n",
      "Episode 1580\tAverage Score: 12.93, with epsilon:0.01000\n",
      "Episode 1590\tAverage Score: 12.89, with epsilon:0.01000\n",
      "Episode 1600\tAverage Score: 12.87, with epsilon:0.01000\n",
      "Episode 1610\tAverage Score: 12.86, with epsilon:0.01000\n",
      "Episode 1620\tAverage Score: 12.95, with epsilon:0.01000\n",
      "Episode 1630\tAverage Score: 13.13, with epsilon:0.01000\n",
      "Episode 1640\tAverage Score: 12.94, with epsilon:0.01000\n",
      "Episode 1650\tAverage Score: 13.15, with epsilon:0.01000\n",
      "Episode 1660\tAverage Score: 13.09, with epsilon:0.01000\n",
      "Episode 1670\tAverage Score: 13.32, with epsilon:0.01000\n",
      "Episode 1680\tAverage Score: 13.18, with epsilon:0.01000\n",
      "Episode 1690\tAverage Score: 13.07, with epsilon:0.01000\n",
      "Episode 1700\tAverage Score: 13.06, with epsilon:0.01000\n",
      "Episode 1710\tAverage Score: 13.05, with epsilon:0.01000\n",
      "Episode 1720\tAverage Score: 13.11, with epsilon:0.01000\n",
      "Episode 1730\tAverage Score: 12.75, with epsilon:0.01000\n",
      "Episode 1740\tAverage Score: 12.65, with epsilon:0.01000\n",
      "Episode 1750\tAverage Score: 12.57, with epsilon:0.01000\n",
      "Episode 1760\tAverage Score: 12.47, with epsilon:0.01000\n",
      "Episode 1770\tAverage Score: 12.04, with epsilon:0.01000\n",
      "Episode 1780\tAverage Score: 12.18, with epsilon:0.01000\n",
      "Episode 1790\tAverage Score: 12.22, with epsilon:0.01000\n",
      "Episode 1800\tAverage Score: 12.40, with epsilon:0.01000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = dqn()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to analyze the result obtained we begin reccaling the reward modification since with this we should expect an average score lower than the non modified version. To overcome this \"issue\", keeping in mind the total number of step is 300 and that the banana collection will be a sparse event, we lowered the threshold to consider the task solved from $13$ to $11$. However we didn't stop the training at the first model cabable to solve the task but we kept learning for all the steps allowed.\n",
    "\n",
    "At the end of the training we can observe that the agend solved its task in exactly 560, however proceding with the training we are capable to reach the best result at episode 1324 reaching also the more strict threshold of 13 for the \"non step penalizing\" reward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total average score: 16.0\n"
     ]
    }
   ],
   "source": [
    "# Test the agent\n",
    "\n",
    "# initialize the score\n",
    "score = 0\n",
    "# load the weights from file\n",
    "agent.qnetwork_local.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    # initialize the environment\n",
    "    state = env.reset(train_mode=False)[brain_name].vector_observations[0] # reset the environment\n",
    "\n",
    "    # for each time step\n",
    "    for j in range(1000):\n",
    "        \n",
    "        action = agent.act(state)\n",
    "        env_info = env.step(action)[brain_name] # send the action to the environment\n",
    "        state = env_info.vector_observations[0] # get the next state\n",
    "        reward = env_info.rewards[0] # get the reward\n",
    "        score += reward # update the score\n",
    "        #if reward != 0:\n",
    "        #    print('\\rEpisode {}: Reward {}\\tTotal Score: {:.2f}'.format(i+1,reward, score))\n",
    "        done = env_info.local_done[0]\n",
    "\n",
    "        if done:\n",
    "            break \n",
    "\n",
    "print(\"Total average score: {}\".format(score/10))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion we tested the final traind agent on the environment observing that on an average of 10 episode it was able to collect an average of 16 banans for episode so we can considered the task completed soccesfuly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video width=\"640\" height=\"480\" controls autoplay loop>\n",
    "  <source src=\"banana.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
