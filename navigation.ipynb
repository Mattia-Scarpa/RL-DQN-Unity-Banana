{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Environment and Agent\n",
    "\n",
    "The first step will be to load the environment built on unity, The we exctract \"_brain_\" information, which is the responsible of taking action in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Banana_Linux/Banana.x86_64\")\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the information of the environment we get that the agent has at its disposal 4 action given by:\n",
    "\n",
    "- `0` - walk forward;\n",
    "- `1` - walk backward;\n",
    "- `2` - turn left;\n",
    "- `3` - turn right.\n",
    "\n",
    "On the other hand the state is made of a 37-dimensional space containing the agent velocity of a ray-based perception for all the objects in the environment around the agent's forward direction.\n",
    "\n",
    "The first step brefore to train our agent is to reset the environment, namely placing it in a random point in its available square space. The final goal for the agent will be to collect as much yellow banana as possibile (which will provide a reward of +1), while avoiding the blue banana (returning a penalty of -1).\n",
    "\n",
    "The environment will be considered solved if the agent will collect at least an average overall reaward &ge;13 along 10 episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time to import the agent responsible to learn the task just described. \n",
    "In order to solve the task the idea is to implement a Deep Q-Network (DQN) that, at each time step or at every pre defined step interval, a neuarl networks (NN) aims to learn the best action, or more formally aims to approximate the state action-value providing a tool to find the more valuable action, to apply at that time istant provided the 37-dimensional state input.\n",
    "\n",
    "At each optimization step the agent will take the action according to the model prediction and will exploit the obtained reward to optimize the NN weigths exploiting the gradiend descent technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqn_agent import Agent\n",
    "\n",
    "\n",
    "action_size = brain.vector_action_space_size\n",
    "state_size = len(env_info.vector_observations[0])\n",
    "\n",
    "agent = Agent(state_size=state_size, action_size=action_size, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Training\n",
    "\n",
    "We must be aware that DQN tends to overestimate action values. To avoid this issues, and to improve the learning phase, we apply some techniques that has been shown to be the more prominent ones.\n",
    "Specifically we will resot to **_Double DQN_** (DDQN), **_Prioritized Experience Replay_** (PER) and, last but not least, **_Dueling DQN_** which combined with DDQN becomes **_Dueling DDQN_**.\n",
    "\n",
    "1. ##### Double DQN\n",
    "\n",
    "Double Q-Learning has shown great performances to avoid the issues of action values overestimation. To do so we start by analyzing the update rule for Q-Learning with NN function approximation, that is:\n",
    "\n",
    "$$\\omega_{i+1} = \\omega_i + \\alpha \\left( R + \\gamma \\max_a Q(s', a'; \\omega_i) - Q(s, a; \\omega_i) \\right) \\nabla_{\\omega_i} Q(s, a; \\omega_i)$$\n",
    "\n",
    "with TD target given by $R + \\gamma \\max_a Q(s', a'; \\omega_i)$ or $R + \\gamma Q(s', argmax_aQ(s', a, \\omega), \\omega)$. Here we can see that if $\\omega$ are not well tuned, for instance at early stage, it might be easy to make mistakes.\n",
    "To overcome this issues in the evaluation of the TD target we will resort to a different set of weights $\\omega^-_i$ that will be updated parallely to the main weights according to the rule given by:\n",
    "$$\\omega^-_i = \\tau * \\omega_i + (1 - \\tau) * \\omega^-_i$$\n",
    "where $\\tau$ is a coefficent that aim to regularize the target update according to the local NN. Namely, if $\\tau = 1$ means that the two networks coincide, and thus the Double DQN becomes a simple DQN, while if $\\tau = 0$ the target NN is not getting updated. This parameter might be an hyperparameters interesting to tune or even changing along the training. However in this project it has been chosen fixed.\n",
    "\n",
    "2. ##### Prioritized Experience Replay\n",
    "\n",
    "In traditional deep reinforcement learning, Experience Replay enhances training by storing and randomly sampling past experiences. However, treating all experiences as equal can be inefficient. For this reason we will exploit Prioritized Experience Replay (PER).\n",
    "\n",
    "PER revolutionizes the sampling process by assigning priorities to experiences based on their temporal difference (TD) errors. Larger TD errors signal unexpected or significant experiences, making them prime candidates for relearning. Instead of sampling uniformly, PER adjusts the probability of selecting a particular experience based on its TD error. Thus, experiences that offer more learning potential are revisited more often.\n",
    "\n",
    "This non-uniform sampling, while efficient, introduces bias. To counteract this, importance-sampling weights are utilized, ensuring unbiased learning updates. In essence, PER combines the best of both worlds: the efficiency of prioritizing valuable experiences and the rigor of unbiased learning.\n",
    "\n",
    "\n",
    "3. ##### Dueling DDQN\n",
    "\n",
    "Dueling DQN introduces an architectural nuance to the DQN's neural network. Rather than directly estimating Q-values, it decouples the state values and the advantages of each action.\n",
    "\n",
    "In a traditional setup, the neural network provides Q-values for actions given a state. With Dueling DQN, the architecture divides the final layer into two pathways: one for the state value function $ V(s; \\omega) $ and another for the advantage function $ A(s, a; \\omega) $ of each action. These are combined to produce Q-values:\n",
    "\n",
    "$Q(s, a; \\omega) = V(s; \\omega) + A(s, a; \\omega)$\n",
    "\n",
    "By separating state value and action advantages, Dueling DQN allows for a more nuanced learning process, leading to potentially faster and more stable policies.\n",
    "\n",
    "\n",
    "#### Other Hyperparameters\n",
    "\n",
    "1. **DDQN Coefficent**: As previously state $\\tau$ is the responsible on tuning the DDQN approach. For this experiment we set the values fixed to $\\tau=5 \\times 10^{-3}$.\n",
    "2. **Learning Rate**: In the DQN's neural network, the learning rate (LR) determines the step size taken towards minimizing the loss during each update. Set initially to $1 \\times 10^{-3}$, it dictates how aggressively our model adapts to new information. Furthermore, the learning rate decays at a rate of 0.9999 after each update, gradually reducing the step size and ensuring more refined learning as training progresses.\n",
    "3. **Discount Factor**: The discount factor $ \\gamma $, set at 0.95, influences how the DQN values future rewards in comparison to immediate ones. A value closer to 1 means the agent gives substantial importance to long-term rewards, ensuring that it's motivated to make decisions beneficial in the long run. This fit our case since the majority of the action are not supposed to return a positive reward or just only a reward different from 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(n_episode = 1800, max_t= 1000, eps_start=1., eps_end=.01, eps_decay=.999):\n",
    "\n",
    "    \"\"\"Deep Q-Learning.\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    scores = []                         # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)    # last 10 scores\n",
    "    eps = eps_start                     # initialize epsilon\n",
    "    best_scores = 11.0\n",
    "\n",
    "    for i_episode in range(1, n_episode+1):\n",
    "        state = env.reset(train_mode=True)[brain_name].vector_observations[0] # reset the environment\n",
    "        #print(state)\n",
    "        score = 0\n",
    "\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps) # select an action\n",
    "            \n",
    "            env_info = env.step(action)[brain_name] # send the action to the environment\n",
    "\n",
    "            next_state = env_info.vector_observations[0] # get the next state\n",
    "            reward = env_info.rewards[0]\n",
    "            # change the reward to -0.01 if the agent doesn't get the banana\n",
    "            if reward == 0:\n",
    "                reward = -0.01\n",
    "            done = env_info.local_done[0]\n",
    "\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "\n",
    "            score += reward # update the score\n",
    "            state = next_state # roll over the state to next time step\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        scores_window.append(score) # save most recent score\n",
    "        scores.append(score)\n",
    "        eps = max(eps_end, eps_decay*eps)\n",
    "\n",
    "        if i_episode % 10 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}, with epsilon:{:.5f}'.format(i_episode, np.mean(scores_window), eps))\n",
    "        if np.mean(scores_window)>=best_scores:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'best_model.pth')\n",
    "            best_scores = np.mean(scores_window)\n",
    "\n",
    "            \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tAverage Score: -2.69, with epsilon:0.95111\n",
      "Episode 20\tAverage Score: -2.69, with epsilon:0.90461\n",
      "Episode 30\tAverage Score: -2.66, with epsilon:0.86038\n",
      "Episode 40\tAverage Score: -2.69, with epsilon:0.81832\n",
      "Episode 50\tAverage Score: -2.75, with epsilon:0.77831\n",
      "Episode 60\tAverage Score: -2.75, with epsilon:0.74026\n",
      "Episode 70\tAverage Score: -2.72, with epsilon:0.70407\n",
      "Episode 80\tAverage Score: -2.56, with epsilon:0.66965\n",
      "Episode 90\tAverage Score: -2.37, with epsilon:0.63691\n",
      "Episode 100\tAverage Score: -2.33, with epsilon:0.60577\n",
      "Episode 110\tAverage Score: -2.18, with epsilon:0.57615\n",
      "Episode 120\tAverage Score: -1.86, with epsilon:0.54799\n"
     ]
    }
   ],
   "source": [
    "scores = dqn()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total average score: 15.6\n"
     ]
    }
   ],
   "source": [
    "# Test the agent\n",
    "\n",
    "# initialize the score\n",
    "score = 0\n",
    "# load the weights from file\n",
    "agent.qnetwork_local.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    # initialize the environment\n",
    "    state = env.reset(train_mode=False)[brain_name].vector_observations[0] # reset the environment\n",
    "\n",
    "    # for each time step\n",
    "    for j in range(1000):\n",
    "        \n",
    "        action = agent.act(state)\n",
    "        env_info = env.step(action)[brain_name] # send the action to the environment\n",
    "        state = env_info.vector_observations[0] # get the next state\n",
    "        reward = env_info.rewards[0] # get the reward\n",
    "        score += reward # update the score\n",
    "        #if reward != 0:\n",
    "        #    print('\\rEpisode {}: Reward {}\\tTotal Score: {:.2f}'.format(i+1,reward, score))\n",
    "        done = env_info.local_done[0]\n",
    "\n",
    "        if done:\n",
    "            break \n",
    "\n",
    "print(\"Total average score: {}\".format(score/10))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video width=\"640\" height=\"480\" controls autoplay loop>\n",
    "  <source src=\"banana.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
